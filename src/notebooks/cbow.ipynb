{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# CBOW Implementation of Word2Vec\n",
    "\n",
    "This is part of the background research that I'm working on for [viberary.pizza](https://viberary.pizza/).\n",
    "\n",
    "## Background \n",
    "\n",
    "[Word2vec](https://arxiv.org/abs/1301.3781) was a critical point in NLP work, building on previous work in dimensionality reduction in NLP such as tf-idf, topic modeling, and latent semantic analysis to reduce vocabulary sizes for computational complexity, and additionally, to add context by embedding similar words in the same latent space. \n",
    "\n",
    "As of 2022, it's almost been superceded by [transformers-based architectures](https://e2eml.school/transformers.html), but it's still worth understanding how it works in a historical context, as well as because there is a fair amount of it [in production in Spark](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.ml.feature.Word2Vec.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Implementation\n",
    "\n",
    "There are numerous word2vec implementations in libraries like Spark and Tensorflow. There is not an exact one in PyTorch, but following[this code](https://github.com/FraLotito/pytorch-continuous-bag-of-words/blob/master/cbow.py), as well as reading about the [architecture here](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0) and [here](https://jalammar.github.io/illustrated-word2vec/),  I was able to implement and understand how it works under te covers\n",
    "\n",
    "\n",
    "Original explanation in [PyTorch implementation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) is here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point, we take our raw data for input. \n",
    "Our [training set for Viberary is some sample the Goodreads input dataset](https://github.com/veekaybee/viberary#input-data-sample), \n",
    "which is a string of text containing the metadata for each unique book\n",
    "id. \n",
    "\n",
    "For a single book id, it will contain the book description book title, etc. So a sample of a single book, will look like this\n",
    "\n",
    "\n",
    "```\n",
    "Raw text: All's Fairy in Love and War (Avalon: Web of Magic, #8) To Kara's astonishment, she discovers that a portal has opened in her bedroom closet and two goblins \n",
    "have fallen through! They refuse to return to the fairy realms and be drafted for an impending war. \n",
    "In an attempt to roust the pesky creatures, Kara falls through the portal, smack into the middle of a huge war.\n",
    "Kara meets Queen Selinda, who appoints Kara as a Fairy Princess and assigns her an impossible task: \n",
    "to put an end to the war using her diplomatic skills.\n",
    "```\n",
    "\n",
    "This is initially stored as a Python string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final goal in learning a Word2Vec model with CBOW is, given an input phrase over a context window, to predict the word that's missing. The context window is how many words before and after the word we care about. So, given the phrase \"Kara falls X the portal\", we should be able to predict that the correct word is \"through.\"\n",
    "\n",
    "We do this in Word2Vec by continuously sampling from the raw text over the context window, where the context window around the word is the X variable and the word itself is the target variable. \n",
    "\n",
    "For the first example, \"Kara falls the portal\" is the context and \"through\" is the response variable. Then we shift the window by 1 word and generate another entry. This is the whole of the [continuous bag of words approach.](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "When we're first training the model, we pass these samples into the model and ask it to make a prediction on a single word given all these samples. The output is a vector of propabilities of the sample related to each word. We then compare that prediction to the actual label (I.e. for the sample \"Kara falls X the portal\" we KNOW the correct word is \"through\").\n",
    "\n",
    "We compare the actual vector (i.e. where through = 1) to the probability vector, and the difference between the two is the loss. The parameters are passed to the model across multiple epochs and continuously updated until we minimize the loss, i.e. we get as close to the predicted word as possible. \n",
    "\n",
    "In the process of doing this prediction, we create a lookup table of words, or embeddings matrix, to their vector representations. It is these vectors that become our embeddings. Andiamo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch  # if you don't have it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need these bois\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll initialize our hyperparameters for the model:\n",
    "\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right - this is our context window\n",
    "EMBEDDING_DIM = 100  # size of the embeddings matrix - we'll get to this in a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tiny training dataset\n",
    "\n",
    "raw_text = \"\"\"To Kara's astonishment, she discovers that a portal has opened in her bedroom closet and two goblins have fallen through! They refuse to return to the fairy realms and be drafted for an impending war. \n",
    "In an attempt to roust the pesky creatures, Kara falls through the portal, \n",
    "smack into the middle of a huge war. Kara meets Queen Selinda, who appoints \n",
    "Kara as a Fairy Princess and assigns her an impossible task: \n",
    "to put an end to the war using her diplomatic skills.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing get only individual words\n",
    "vocab = set(raw_text)  # dedup\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create simple mappings of word to an index of the word\n",
    "word_to_ix = {word: ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix: word for ix, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our training data and context window\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1], raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))"
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# We have our [input, input, input, input, target]\n",
     "# based on the context window of +2 words -2 words\n",
     "# you can see how we're building words close to each other now\n",
     "print(*data[0:10], sep=\"\\n\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## Model Set Up\n",
     "# CBOW Architecture \n",
     "\n",
     "We have two layers in the CBOW implementation of Word2Vec: an input Embedding layer that maps each word to a space in the embedding dictionary, a hidden linear activation layer, and then the output layer that is the proportional probabilities [softmax](https://en.wikipedia.org/wiki/Softmax_function) of all the correct words given an input window. \n",
     "\n",
     "The critical part is the first part, creating the Embeddings lookup. \n",
     "\n",
     "First, we associate each word in the vocabulary with an index, aka `{'she': 0, 'middle': 1, 'put': 2`\n",
     "\n",
     "Then, what we want to do is create an embeddings table, or matrix, that we will multiply with these indices to map each one to its correct place in relation to the other indices via a table lookup, based on how many vectors you'd like to represent the word. \n",
     "\n",
     "There is a [really good explanation](https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work/305032#305032) of how these are generated: \n",
     "\n",
     "For a given word, you create a one-hot vector based on its index and multiply it by the embeddings matrix, effectively replicating a lookup. For instance, for the word \"soon\" the index is 4, and the one-hot vector is [0, 0, 0, 0, 1, 0, 0]. If you multiply this (1, 7) matrix by the (7, 2) embeddings matrix you get the desired two-dimensional embedding, which in this case is [2.2, 1.4].\n"
    ]
   },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim\n",
    "    ):  # we pass in vocab_size and embedding_dim as hyperparams\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        # out: 1 x embedding_dim\n",
    "        self.embeddings = nn.Embedding(\n",
    "            vocab_size, embedding_dim\n",
    "        )  # initialize an Embedding matrix based on our inputs\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "\n",
    "        # out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1, -1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        # Embeddings lookup of a single word once the Embeddings layer has been optimized\n",
    "        return self.embeddings(word).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the model:\n",
    "\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, we initialize the loss function\n",
    "# (aka how close our predicted word is to the actual word and how we want to minimize it using the optimizer)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# 50 to start with, no correct answer here\n",
    "for epoch in range(50):\n",
    "    # we start tracking how accurate our intial words are\n",
    "    total_loss = 0\n",
    "\n",
    "    # for the x, y in the training data:\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        # we look at loss\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        # we compare the loss from what the actual word is related to the probaility of the words\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "\n",
    "    # optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log out some metrics to see if loss decreases\n",
    "    print(\"end of epoch {} | loss {:2.3f}\".format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's test to see if the model predicts the correct word using our initial input\n",
    "context = [\"Kara\", \"falls\", \"the\", \"portal\"]\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Raw text: {\" \".join(raw_text)}\\n')\n",
    "print(f\"Context: {context}\\n\")\n",
    "print(f\"Prediction: {ix_to_word[torch.argmax(a[0]).item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get what we care about, which is the embeddings!\n",
    "print(f\"Getting vectors for a sequence:\\n\", model.embeddings(torch.LongTensor([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Getting weights:\\n\", model.embeddings.weight.data[1]\n",
    ")  # we can get the entire matrix this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, what we actually care about is being able to look up individual words with their embeddings:\n",
    "torch.set_printoptions(threshold=10_000)\n",
    "print(f\"Embedding for Kara: {model.embeddings.weight[word_to_ix['Kara']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This way, when we create our second tower of book words, we know which ones are likely related to a given book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7983be674d93518c54b39475eb68739ef6a55aa8f4ec8a69dd7da1e80860d970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
